{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Neural Network\n",
    "## 5. 출력층 설계하기\n",
    "\n",
    "> - 출력층 활성화 함수는 **풀고자 하는 문제의 성질에 맞게** 정의\n",
    "- i.e. 회귀 - 항등함수, 2클래스 분류 - 시그모이드함수, 다중 분류 - 소프트맥스함수\n",
    "\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 항등 함수와 소프트맥스 함수 구현하기\n",
    "\n",
    "- 항등 함수(identity function)은 입력을 그대로 출력 $\\sigma()$\n",
    "\n",
    "<br>\n",
    "- 소프트맥스 함수(softmax function)는 아래의 식과 같음\n",
    "$$\n",
    "y_k = \\frac{exp(a_k)}{\\sum_{i=1}^{n}exp(a_i)}\n",
    "$$\n",
    "\n",
    "- 여기서 $n$은 출력층의 뉴런 수, $k$는 $k$번째 출력임을 의미\n",
    "- 소프트맥스 함수의 분자는 입력신호 $a_k$의 지수함수, 분모는 모든 입력신호의 지수함수의 합으로 구성\n",
    "- 다시 이야기 하면, **'활성화되기 전의 출력값들을 전체와의 비율로 나타내주는 활성화 함수'**\n",
    "- 아래 그림 참조\n",
    "![](image/fig 3-22.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소프트맥스 함수를 한 단계씩 구현하면 아래와 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.34985881 18.17414537 54.59815003]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([.3, 2.9, 4])\n",
    "exp_a = np.exp(a)\n",
    "print(exp_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_exp_a = exp_a.sum()\n",
    "print(sum_exp_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = exp_a / sum_exp_a\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 내용을 파이썬 함수로 정의 시 다음과 같이 정의 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = exp_a.sum()\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### 2) 소프트맥스 함수 구현 시 주의점\n",
    "\n",
    "- 위의 softmax 함수엔 오버플로우의 위험이 있음\n",
    "- softmax 함수는 지수함수를 사용하는 만큼 매우 큰 값을 반환하고, 때로는 inf 값을 반환하기도 함\n",
    "- 큰 수 간의 나눗셈으로 인해 수치가 '불안정'해 질 수도 있음\n",
    "- i.e. $e^{10}$은 20,000이 넘고, $e^{100}$만 되어도 0이 40개가 넘는 큰 값이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이를 해결하기 위해 softmax 함수 구현을 다음과 같이 개선 가능\n",
    "\n",
    "$$\n",
    "y_k = \\frac{exp(a_k)}{\\sum_{i = 1}^nexp(a_i)} = \\frac{Cexp(a_k)}{C\\sum_{i=1}^nexp(a_i)}\\\\\n",
    "= \\frac{exp(a_k + logC)}{\\sum_{i=1}^nexp(a_i + logC)}\\\\\n",
    "= \\frac{exp(a_k+C')}{\\sum_{i=1}^nexp(a_i+C')}\n",
    "$$\n",
    "\n",
    "1. 첫 번째 변형에서는 $C$라는 임의의 정수는 분자와 분모 양쪽에 곱함\n",
    "- $C$를 지수함수 안으로 옮겨 $logC$로 변환\n",
    "- $logC$를 $C'$로 치환\n",
    "\n",
    "\n",
    "- $C'$에 어떤 값을 대입해도 상관 없지만, **입력신호 중 최댓값**을 이용하여 오버플로우를 막는 것이 일반적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1010, 1000, 990])\n",
    "np.exp(a) / np.exp(a).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.max(a)\n",
    "a - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(a-c) / np.exp(a-c).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 예시처럼 아무런 조치 없이 입력값에 큰 값이 들어가게 된다면 오버플로우로 인해 nan이 출력됨\n",
    "- 하지만 입력 신호 중 최댓값을 빼주는 것만으로 올바르게 구현 가능\n",
    "- 아래는 softmax 함수를 수정한 버전임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = a.max()               #오버플로우 방지\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = exp_a.sum()\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)\n",
    "softmax(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### 3) 소프트맥스 함수의 특징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([.3, 2.9, 4])\n",
    "y = softmax(a)\n",
    "print(y)\n",
    "np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- softmax 함수의 출력은 0에서 1사이의 실수이며, **총합은 1**임\n",
    "- 총합이 1이라는 성질 덕분에 softmax 함수의 출력을 **확률**로 해석할 수 있음\n",
    "- 앞의 예에서 y[2]의 확률이 73.7%로 가장 높으니 답이 2번째 클래스일 가능성이 높다고 결론 내릴 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망 분류에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식\n",
    "- 따라서 출력층의 소프트맥스 함수는 생략하는 편 (자원낭비 방지)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 출력층의 뉴런 수 정하기\n",
    "- 출력층의 뉴런 수는 풀려는 문제에 맞게 설정\n",
    "- 분류에서는 분류하고 싶은 클래스 수로 설정\n",
    "![](image/fig 3-23.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
